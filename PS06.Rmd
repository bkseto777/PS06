---
title: "STAT/MATH 495: Problem Set 06"
author: "Brendan Seto"
date: "2017-10-17"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    collapsed: false
    smooth_scroll: false
    df_print: kable
    code_fold: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE
  )
set.seed(76)

# Load packages
library(tidyverse)
library(broom)
library(knitr)
library(mosaic)
```




# Setup

Define truth, which again we know for the purposes of this assignment, but in
practice we won't:

* the true function f(x) i.e. the signal
* the true epsilon i.e. the noise, which in this case is Normal$(0, sd=\sigma)$.
Hence the standard deviation $\sigma$ determines the amount of noise.

```{r}
f <- function(x) {
  x^2
}
sigma <- 0.3
```

This is the target point we'll be trying to predict: $(0.95, f(0.95)) = (0.95, 0.95^2) = (0.95, 0.9025)$, Thus, the test set is just `x=0.95`

```{r}
x0 <- 0.95
test_set <- data_frame(x=x0)
```

This function generates a random sample of size $n$; think of this as a "get new
data" function. Random in terms of both:

* (New) the predictor x (uniform on [0,1])
* the amount of noise $\epsilon$

```{r}
generate_sample <- function(f, n, sigma) {
  sample <- data_frame(
    x = runif(n = n, min = 0, max = 1),
    f_x = f(x),
    epsilon = rnorm(n = n, mean = 0, sd = sigma),
    y = f_x + epsilon
  )
  # Recall: We don't observe f(x) and epsilon, just (x, y)
  sample <- sample %>% 
    select(x, y)
  
  return(sample)
}
```

Define

* The number $n$ of observations $(x_i, y_i)$ in each sample. In the handout,
$n=100$ to keep plots uncrowded. Here we boost to $n=500$
* Number of samples of size $n$ to consider

```{r}
n <- 500
n_sim <- 10000
```


# Computation

```{r}
# This function generates a sample and runs either a linear (df=2) or spline (df = 99) model on it
# It then used the model to create a predicted y_hat for x0, 0.95.  
yhat <- function(df, n){
  sample <- generate_sample(f, n, sigma)
  m1 <- with(sample, smooth.spline(x, y, df = df))
  output <- predict(m1, 0.95)
  output$y
}


# This function generates n_sim y_hats
# It then calculates MSE, the var(y_hat), and the bias(y_hat)^2
var_bias <- function(df){
  hi <- do(n_sim)*yhat(df, n)
  hi <- hi %>%   mutate(y_data = rnorm(n_sim, mean = f(0.95), sd = sigma), SE = (y_data-yhat)^2)
  
  output <- c(df, mean(hi$SE), var(hi$yhat), (mean(hi$yhat)-f(x0))^2)
  return(output)
}


testing <- var_bias(2)

testing <- rbind(testing, var_bias(99))
testing <- as.data.frame(testing)
colnames(testing) <- c("df","MSE","var","bias_squared")
testing <- testing %>% mutate(irreducible = sigma^2, sum = var + bias_squared + irreducible)
testing <- data.frame(Model = c("Linear", "Spline"),testing)

testing %>% knitr::kable(digits = 4)
```


# Analysis

**Questions**:

1. Based on the topics covered in Lec 2.7, name one possible "sanity check" for your results. Name another if you can.
2. In **two** sentences or less, give a rough sketch of what the procedure would
be to get the breakdown of $$\mbox{MSE}\left[\widehat{f}(x)\right]$$ for *all*
$x$ in this example, and not just for $$\mbox{MSE}\left[\widehat{f}(x_0)\right]
= \mbox{MSE}\left[\widehat{f}(0.95)\right]$$.
3. Which of the two models would you choose for predicting the point of interest and why?

**Answers**:

1. The first sanity check is to ensure that the bias-variance tradeoff holds.  It does.  This is evident by the fact that the variance of the linear model is small and its bias is large, where as the opposite is true for the spline model.  A second check is to see if the MSE and $variance + bias^2 + \epsilon$ are similar to eachother for the same model.  This would further showcase the fact that our MSE calculations are correct.  

2. The process would be very similar, except each MSE, variance and bias would be generated from many more predicted y_hats.  Instead of using 10,000 predicted values you would use 10,000 * the size of the domain X.  

3. I would choose the linear model.  Underfitting the model and losing some of the signal is perferable to overfitting and losing reliability.  Even if you know the spline model is on average more accurate, it's less likely to be extremely off.  This of course depends somewhat on what the prediction is being used for and how risk averse I need to be at the time.  

